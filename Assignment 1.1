Explain the following terms in detail:
          1) VARIOUS SOURCES OF BIG DATA
          ANS: 
              1.Archives-Internal archived data that lives behind your own firewalls is typically unstructured, and uses no APIs (unless you digitize them).
              2.DOCS- can exist inside or outside your organization, and like archived data, doesn’t use APIs.
              3.Media- It exists in-and-out of your organization, may connect with APIs (think an API to collect images from Pinterest and embed them into a product page or email merchandising) and is moderately structured.
              4.Data Storage- SQl,NOSQL,Hadoop,Doc Repository,File Systems.
              5.Business Applications- They are structured, and using APIs you can pull data from both inside and outside your organization. Internally, think integrating your CRM or Web Content Management with your ecommerce system.
              6.Public Web-Government,Weather,traffic,health care services,census,public finance stock.
              7.Social Media - It is high velocity, high volume data that you can use to detect trends, analyze sentiment about your brand, customer service and competitors, or target campaigns to social accounts that match the email addresses in your customer file (
              8.Machine Log Data - Event logs,Server data,application logs,audit logs,call detail records.
              9.Sensor Data-It is high velocity, volume, variety and dare I add…value, when used correctly to understand user context and predict behavior. Sensors for geolocation, temperature, noise, attention, engagement, biometrics, and more can collect reams of data that is useful for better purchase and ownership experiences in a variety of industries.
               
           2) 3V's of Big Data     
           ANS:
               Volume refers to the amount of data, variety refers to the number of types of data and velocity refers to the speed of data processing.
               Data Volume:
                      The size of available data has been growing at an increasing rate. This applies to companies and to individuals. A text file is a few kilo bytes, a sound file is a few mega bytes while a full length movie is a few giga bytes.
                      More sources of data are added on continuous basis. For companies, in the old days, all data was generated internally by employees. Currently, the data is generated by employees, partners and customers. For a group of companies, the data is also generated by machines. For example, Hundreds of millions of smart phones send a variety of information to the network infrastructure. This data did not exist five years ago.
                      More sources of data with a larger size of data combine to increase the volume of data that has to be analyzed. This is a major issue for those looking to put that data to use instead of letting it just disappear.
                      Peta byte data sets are common these days and Exa byte is not far away.
               Data Velocity:
                     Initially, companies analyzed data using a batch process. One takes a chunk of data, submits a job to the server and waits for delivery of the result. 
                     That scheme works when the incoming data rate is slower than the batch processing rate and when the result is useful despite the delay. With the new sources of data such as social and mobile applications, the batch process breaks down. 
                     The data is now streaming into the server in real time, in a continuous fashion and the result is only useful if the delay is very short.
               Data Variety:
                     From excel tables and databases, data structure has changed to loose its structure and to add hundreds of formats. Pure text, photo, audio, video, web, GPS data, sensor data, relational data bases, documents, SMS, pdf, flash, etc etc etc. 
                     One no longer has control over the input data format. Structure can no longer be imposed like in the past in order to keep control over the analysis. As new applications are introduced new data formats come to life.      
                     
                     
              3) Horizontal Scaling and Vertical Scaling       
               ANS:
                    Horizontal scalability-It is the ability to increase capacity by connecting multiple hardware or software entities so that they work as a single logical unit. 
                    An important advantage of horizontal scalability is that it can provide administrators with the ability to increase capacity on the fly. Another advantage is that in theory, horizontal scalability is only limited by how many entities can be connected successfully. The distributed storage system Cassandra, for example, runs on top of hundreds of commodity nodes spread across different data centers. Because the commodity hardware is scaled out horizontally, Cassandra is fault tolerant and does not have a single point of failure (SPoF).
                    
                    Vertical scalablity-When an existing IT resource is replaced by another with higher or lower capacity, vertical scaling is considered to have occurred (Figure 2). Specifically, the replacing of an IT resource with another that has a higher capacity is referred to as scaling up and the replacing an IT resource with another that has a lower capacity is considered scaling down. Vertical scaling is less common in cloud environments due to the downtime required while the replacement is taking place.
                4)Need and Working of Hadoop
                ANS:
                     Need:It enables surplus data to be streamlined for any distributed processing system across clusters of computers using simple programming models. It truly is made to scale up from single servers to a large number of machines, each and every offering local computation, and storage space. Instead of depending on hardware to provide high-availability, the library itself is built to detect and handle breakdowns at the application layer, so providing an extremely available service along with a cluster of computers, as both versions might be vulnerable to failures.
                     Working: The way HDFS works is by having a main « NameNode » and multiple « data nodes » on a commodity hardware cluster. All the nodes are usually organized within the same physical rack in the data center. Data is then broken down into separate « blocks » that are distributed among the various data nodes for storage. Blocks are also replicated across nodes to reduce the likelihood of failure.
